<h1 align="center">RL基础知识 & GRPO公式推导</h1>

# RL基础概念：S、A、R、π、环境

## 状态s、动作a、奖励r、策略π

在用强化学习训练LLM的语境中，有两种建模级别。
- token级建模：每个生成的token视为一个action（对应PPO）
- sequence级建模：每个完整生成序列视为一个action（对应GRPO）

相应的，把强化学习的概念对应到LLM上：
**token级建模**
- 状态s：
  - 初始状态 $s_1$：prompt $x$
  - t时刻状态 $s_t$：prompt $x$ + response的前t个token $y_{1:t}$
  - （此处 $y_i$ 指的是序列 y 的第 i 个token）
- 动作a：
  - t时刻的动作 $a_t$：response的第t个token $y_{t}$
- 奖励r：
  - 最后一个token：完整序列的奖励（例如答案是否正确）
  - 前面的token：奖励为0
- 策略π：
  - LLM生成第t个token的概率 $P(y_t | x, y_{1:t-1})$

**sequence级建模（只有一问一答）**
- 状态s：只有初始、结束两个状态
  - 初始状态 $s_1$：prompt $x$
  - 结束状态 $s_2$：prompt $x$ + response $y$
- 动作a：只有一步动作
  - LLM生成的response $y$
- 奖励r：
  - 完整序列的奖励（例如答案是否正确）
- 策略π：
  - LLM生成response y的概率 $P(y | x)$

**sequence级建模（多轮问答）**：常见于多轮agent的RL训练，但也可以全拆成单轮
- 状态s：
  - 初始状态 $s_1$：初始prompt $x_{1}$
  - t时刻状态 $s_t$：初始prompt $x_{1}$ + 第1轮response $y_{1}$ + ... + 第t轮prompt $x_{t}$ + 第t轮response $y_{t}$
  - （此处的 $x_i,y_i$ 指代的是第 i 个 x,y 序列，而不是序列中第i个token）
- 动作a：
  - t时刻的动作 $a_t$：第t轮response $y_{t}$
- 奖励r：
  - t时刻的奖励 $r_t$：第t轮回复的奖励（有些方法有中间奖励），如果t是最后一轮则对应终局奖励
- 策略π：
  - LLM生成第t个response的概率 $P(y_t | x_1, y_1, ..., x_{t-1}, y_{t-1}, x_t)$
 
## 环境：确定性环境 & 概率环境

环境描述了给定状态s，执行动作a时，会转移到什么新状态s'，并且会获得多少奖励r。
- 有时s'和r都是确定的，则环境是确定性环境
- 有时s'和r是有随机性的，则环境是概率环境，此时需要用概率分布$P(s',r|s,a)$来描述

具体而言又可以分为s'有随机性、r有随机性等多种不同情况。以下以“步枪打靶”为例说明。

> 步枪打靶游戏：让步枪瞄准某个坐标并开枪，打中则获奖。只有一步决策，初始/结束两个状态
> - 初始状态s：步枪当前瞄准坐标 $(x_1,y_1)$
> - 动作a：“令步枪瞄准新坐标 $(x_2,y_2)$ 并开枪”的指令（把移动和开枪视为一整个动作）
> - 结束状态s'：步枪实际瞄准的新坐标 $(x'_2,y'_2)$ 
> - 奖励r：打中 $(x_2,y_2)$ 位置的靶则奖励1，否则为0
>
> 随机性的来源：
> - s'的随机性：动作执行可能不准确，命令枪瞄准$(x_2,y_2)$ 和枪实际瞄准 $(x'_2,y'_2)$ 可能并不相同
> - r的随机性：动作执行产生的效果可能不稳定，枪就算能瞄准 $(x_2,y_2)$，子弹可能会偏（瞄准了却没打中、瞄歪却打中了）
> 
> 具体解释：
> - s'随机，r确定：坏枪+好子弹，枪不一定能准确瞄准靶子，但子弹一定能准确打到瞄准的位置
> - s'确定，r随机：好枪+坏子弹，枪一定能准确瞄准靶子，但子弹可能偏到其他位置
> - s'随机，r随机：坏枪+坏子弹，枪瞄的不准，子弹落点也不准
> - s'确定，r确定（确定性环境）：好枪+好子弹在

## 强化学习中的随机性：策略随机性 vs 环境随机性

强化学习中的随机性有两个来源：
- 策略随机性 $π(a|s)$：给定状态s，以根据某种概率分布随机生成动作a。（与之相对的是最优性策略：给定s，生成某个“最优”的a）
- 环境随机性 $P(s',r|s,a)$：给状态s和动作a，转移到的新状态s'或获得的奖励r有随机性（与之相对的是确定性环境，转移到的s'和r都确定）

标准的RL各种公式（例如贝尔曼方程）中，是按策略、环境都随机给出的。
但在LLM强化学习的语境下：
- s：prompt+已经生成的token；a：新token或新response序列
- 策略随机性：对于top-p解码，策略是随机的；对于贪婪解码，策略是确定的
- 环境随机性：s无随机性（给定s和a，s'=s拼接a）；r如果由规则判断则是确定的，如果是llm-as-judge可能有随机性（本文暂不考虑这种情况）

对于LLM的强化学习（策略随机、环境确定），比标准的强化学习（策略随机、环境随机）的设定更简单，因此各种公式会有一些简化。

## 马尔科夫性
todo

# 衍生概念：G、V、A、Q

## 轨迹trace（或称episode）τ
$τ=(s_1, a_1, r_1, s_2, a_2, r_2, s_3, ..., s_n, a_n, r_n, s_{n+1})$ 从起始到结束称为一个轨迹。
- 第t步的初始状态为 $s_t$， 执行动作 $a_t$ ， 此时立刻获得奖励 $r_t$ ，并转移到下个状态 $s_{t+1}$
- 一共执行t=1...n，共n次行动
- $τ \sim π(a|s)$ 代表从状态s开始，根据策略 π(a|s) 依次采样下一个动作，直到终局，所产生的轨迹，即“从概率π(a|s)采样的轨迹τ”
  - “从状态s开始”不代表s必须是第一个状态$s_1$，它可以是任何一个中间状态 $s_t$。根据马尔科夫性， $s_t$之后的演进只和 $s_t$有关，与之前如何来到 $s_t$ 无关。

## 收益（gain）G 与折扣因子 $γ$
第t步的收益 $G_t = r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ... = \sum_{i=0}^{\infty} γ^i r_{t+i}$
- 折扣因子 0≤γ≤1
- 含义：从第t步开始直到终局，累计reward之和（越未来的步骤通常越“不重要”，因此使用折扣γ衰减。γ=0 则只看当前动作的reward，γ=1 则每一步的reward都同等重要）
- G默认是到终局位置，但也可以计算k步收益： $G_t^{(k)} = r_{t} + γ r_{t+1} + ... + γ^{k-1} r_{t+k-1} = \sum_{i=0}^{k-1} γ^i r_{t+i}$，它在k步时序差分中会用到（本文不介绍）
- 收益（gain）有时也称作回报（return），两者在强化学习中含义相同，只是不同地方的术语有差异。这里称呼为gain，防止return和reward相似混淆

## 价值（value）函数 $V^π(s)$：
对于策略π，状态s的价值为：
```math
\begin{aligned}
V^π(s) 
& = \mathbb{E}_{τ \sim π(a|s)}[G_t | S_t=s]  \\
& = \mathbb{E}_{τ \sim π(a|s)}[r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ... | S_t=s]
\end{aligned}
```
- 其中状态s执行动作a后，获得奖励r，状态变为s'
- **物理意义**：从状态s按策略π(a|s)采样直到终局，**预期**能累积到多少奖励。

几点注意事项：
- 这里大写的 $S_t$ 表示第t个状态的“占位符”，小写的 $s$ 代表状态的实际取值，可以类比“类vs实例”，正好也是类大写、实例小写
- $V^π(s)$ 和 s 处于序列的第几步无关，只和s与π有关
- 对于不同的策略π，同一个状态s的价值 $V^π(s)$ 可能不同
- $s_t$ 的价值是从 $s_t$ 的**后续**奖励 $r_t$ 开始算的，而不是从到达s时产生的奖励 $r_{t-1}$ 开始算的

**辨析：奖励r、收益G、价值V**
> - 奖励reward：第t步行动**实际产生**的**即时**“收获”，衡量**状态s+动作a**的好坏
> - 收益gain：从第t步到终局**实际产生**的**累计**“收获”，衡量**轨迹τ**的好坏
> - 价值V：即从s到终局时，**预期会产生**的**累计**“收获”，衡量**状态s**的好坏

**为什么引入价值V（为什么只靠奖励r不够）？**
- reward稀疏：有些场景，中间的过程没有奖励，只有终局时才能一次性结算奖励（LLM生成过程就很典型，生成完才能评估是否正确，单个token没有奖励）
- reward好坏 ≠ 动作/状态好坏：即刻的奖励高，可能后续的奖励少，不一定是个好动作/好状态。

因此reward是“局部的、即刻的”，价值V是“全局的、考虑后续的”，它能补齐reward的视角：
- reward稀疏：没有reward时，价值V依然能评估出状态的好坏，并且根据转移到下个状态的V评估动作好坏
- reward好坏 ≠ 动作/状态好坏：考虑对未来的预期，平衡即刻的reward数值

关于价值V本身的一些澄清：**价值V是客观的 → 客观但不可直接获取 → 不可获取但可以估计 → 估计有误差 → 误差可以收敛**  
逐句解释：
- 价值是客观的：给定策略π和状态s，价值 $V^π(s)$ 一定是个客观实际的值。（类比：概率分布的期望是个客观实际的值）
- 客观但不可直接获取：reward和gain是可以直接获取的，但价值V是**期望**，它不能直接获取。（类比：样本的值能直接获取，但背后的期望不能直接获取）
- 不可获取但可以估计：有大量真实数据时，可以构造统计量来近似估计价值 $V^π(s)$。（类比：真实数据的平均数是个人工构造的统计量，可以近似估计概率分布的期望）
  - 动态规划法中可以为V随机赋值，并且迭代解不动点
  - PPO中用神经网络计算V
  - 蒙特卡洛中构造统计量估计V
- 估计有误差：根据真实数据估计出的价值V，数值上和客观实际的V不一定恰好相等。（类比：数据的平均数≈概率的期望，但不一定恰好相等）
- 误差可以收敛：数据足够多/迭代步数足够多，估计出的V和客观实际的V误差可忽略。（类比：数据的平均数≈概率的期望，在数据足够多时误差可忽略）

## 动作价值函数 $Q^π(s,a)$

对于策略π，在状态s下，动作a的价值为：
```math
\begin{aligned}
Q^π(s,a) 
& = \mathbb{E}_{τ \sim π(a|s)}[G_t | S_t=s, A_t=a]  \\
& = \mathbb{E}_{τ \sim π(a|s)}[r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ... | S_t=s, A_t=a]
\end{aligned}
```
- 其中状态s执行动作a后，获得奖励r，状态变为s'
- **物理意义**：从状态s**明确执行动作a**后，未来再按策略π采样直到终局，**预期**能累积到多少奖励。
  - Q也要从s执行a之后产生的奖励开始计算，这点和V的计算一致

Q和V的差异与联系：
- $Q^π(s,a)$衡量**状态s下动作a**的好坏，即状态s**明确采取动作a**后的预期收获（只是说在这一步采取动作a，后续动作还是按策略π采样）
- $V^π(s)$ 衡量**状态s**的好坏，即状态s按策略π随机动作，并且将这些**动作的效果累加**后的预期收获
  - 也就是**贝尔曼方程**： $V^π(s) = \sum_{a} π(a|s) Q^π(s, a)$
  - 贝尔曼方程反映了V和Q的关系
  - 贝尔曼方程还有很多其他形式，这里不展开
  - 这里介绍贝尔曼方程，只是为了解释V和Q的关系，GRPO不涉及贝尔曼方程


## 优势（advantage）函数 $A^π(s,a)$

在给定状态s下，动作a的优势为：$A^π(s,a) = Q^π(s,a) - V^π(s)$
- 借助贝尔曼方程辅助理解：$A^π(s,a) = Q^π(s,a) - \sum_{a} π(a|s) Q^π(s, a)$
  - 其中 $V^π(s) = \sum_{a} π(a|s) Q^π(s, a)$
- 从贝尔曼方程可以看出，优势 $A^π(s,a)$ 的**物理意义**：在状态s下，明确采取动作a，比按策略π随机采取一个动作，产生的**额外**的预期收益
  - $A^π(s,a)>0$：a是个好动作
  - $A^π(s,a)<0$：a是个坏动作
  - $A^π(s,a)=0$：a不好不坏
- 数学性质：给定状态s，所有动作a的收益 $A^π(s,a)$ 之和为0
  - 因为A衡量的是一个动作相比于所有动作的“相对值”，动作有好有坏，有的相对收益为正，就一定有的相对收益为负（相对收益、额外收益是一回事）
- 这个优势A和GPRO中的优势A是**同一个东西**，后面会推导。

## 拓展内容：贝尔曼方程（V←V、Q←Q、V←Q、Q←V 的互相递归计算）

推导蒙特卡洛、GRPO不需要贝尔曼方程，此处略。推导时序差分、PPO才需要。

<details>
<summary><b>点击展开贝尔曼方程的内容</b></summary>
  
**概率环境**：
```math
\begin{aligned}
% 1. V计算V
V^π(s) & = \sum_{a} π(a|s) \sum_{s', r} P(s', r|s, a) [ r + γ V^π(s') ] \\
% 2. Q计算Q
Q^π(s, a) & = \sum_{s', r} P(s', r|s, a) [ r + γ \sum_{a'} π(a'|s') Q^π(s', a') ] \\
% 3. 用Q计算V
V^π(s) & = \sum_{a} π(a|s) Q^π(s, a) \\ 
% 4. 用V计算Q 
Q^π(s, a) & = \sum_{s', r} P(s', r|s, a) [ r + γ V^π(s')]
\end{aligned}
```

**确定性环境**：去掉所有 $P(s',r|s,a)$
```math
\begin{aligned}
% 1. V计算V
V^π(s) & = \sum_{a} π(a|s) [ r + γ V^π(s') ] \\
% 2. Q计算Q
Q^π(s, a) & = r + γ \sum_{a'} π(a'|s') Q^π(s', a') \\
% 3. 用Q计算V
V^π(s) & = \sum_{a} π(a|s) Q^π(s, a) \\ 
% 4. 用V计算Q 
Q^π(s, a) & = r + γ V^π(s')
\end{aligned}
```

</details>  

# 蒙特卡洛：估计V/Q/A

前面介绍价值V时有解释：**价值V是客观的 → 客观但不可直接获取 → 不可获取但可以估计 → 估计有误差 → 误差可以收敛**，这对于Q和优势A也同样成立。

估计V、Q、A的方法很多，其中蒙特卡洛法MC是最直观的。蒙特卡洛的结果带入LLM的场景中做简化，可以直接推导出GRPO中优势A的公式（下一章介绍）

> 除了蒙特卡洛，估计V/Q/A还有其他方法。例如：
> - 动态规划：本质是为V/Q/A随机赋值后，迭代法求不动点。但只适合状态少、动作少的离散toy场景，此处不介绍
> - 时序差分TD：这个分支最终演化为PPO的优势估计（演化过程：时序差分 → 多步时序差分 → 广义优势估计GAE → PPO中的优势A），以后讲PPO时再介绍

蒙特卡洛的本质：生成一堆数据，拿到其中的reward，“直接计算” V/Q/A
  - 类比：采样一堆数据，用它们的均值“直接计算”分布的期望

**回顾V/Q/A的定义：**
```math
\begin{aligned}
G_t &= r_{t} + γ r_{t+1} + γ^2 r_{t+2} + γ^3 r_{t+3} + ...  \qquad \text{where  0≤γ≤1} \\
V^π(s) & = \mathbb{E}_{τ \sim π(a|s)}[G_t | S_t=s] \\
Q^π(s,a) & = \mathbb{E}_{τ \sim π(a|s)}[G_t | S_t=s, A_t=a] \\
A^π(s,a) &= Q^π(s,a) - V^π(s)
\end{aligned}
```

**蒙特卡洛算法：**
- 依据策略π，采样出大量 $τ=(s_0, a_1, r_1, s_1, a_2, r_2, s_2, ...)$ 轨迹，也就是rollout
- 对于某个状态 $s$ 和 动作 $a$ ：
  - 估计 $V^π(s)$ ：
    - 找到所有包含状态 $s$ 的轨迹，轨迹数量记为 $N(s)$
    - 对于每条这样的轨迹，截取从 $S_t = s$ 一直到结尾的轨迹片段，抽取其中的奖励 $r_{t+1}, r_{t+2}, ...$，计算该片段的gain $G_t = r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ...$
    - 将所有这样轨迹的gain取平均： $\hat{V}^π(s)=\frac{\sum_{包含s的τ} G_t}{N(s)}$
  - 估计 $Q^π(s,a)$ ：
    - 找到所有包含状态-动作对 $(s,a)$ 的轨迹，轨迹数量记为 $N(s,a)$
    - 对于每条这样的轨迹，截取从 $(S_t, A_{t+1}) = (s,a) $ 一直到结尾的轨迹片段，抽取其中的奖励 $r_{t+1}, r_{t+2}, ...$，计算该片段的gain $G_t = r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ...$
    - 将所有这样轨迹的gain取平均： $\hat{Q}^π(s,a)=\frac{\sum_{包含(s,a)的τ} G_t}{N(s,a)}$
  - 估计 $A^π(s,a)$ ：
    - $\hat{A}^π(s,a) = \hat{Q}^π(s,a) - \hat{A}^π(s)$

上述三个统计量 $\hat{V}^π, \hat{Q}^π, \hat{A}^π$ ，就是对 $V^π, Q^π, A^π$ 的估计。这个估计高方差、无偏差（与此相对，时序差分对 $V^π, Q^π, A^π$ 的估计是低方差、有偏差的。以后PPO中介绍，此处略）

# GRPO中的优势A：简化环境下的蒙特卡洛

回到强化学习训练LLM的场景。我们按如下方法定义强化学习问题：

**sequence级建模（只有一问一答）**
- 只有一个初始状态、一个结束状态、一个动作
- 完整轨迹：只有 $τ=(s, a, r, s')$
  - 初始状态 $s$：prompt $x$
  - 动作 $a$：LLM生成的完整response $y$，（动作空间是无限的）
  - 奖励 $r$：完整序列的奖励（例如答案是否正确）
  - 结束状态 $s'$：prompt $x$ + response $y$
  - 策略 $π(a|s)$： LLM生成response y的概率 $P(y | x)$
  - 环境的随机性 $P(s',a'|s,a)$：环境是确定的，没有随机性

**蒙特卡洛估计**：
  - rollout：取一个prompt $x$，生成N个不同的response，对应n种不同的动作
    - 记 $y^i$ 为第i个rollout， $r^i$ 为对应的奖励
  - 估计V、Q、A：
    - 因为只有一步奖励 $r$，所以收益 $G_t=r$，不用考虑后续的衰减γ
    - $\hat{V}^π(x) = \frac{\sum_{i=1}^{N} G_t^i}{N} = \frac{\sum_{i=1}^{N} r^i}{N} = μ$，即奖励 $r$ 的平均值 μ
    - $\hat{Q}^π(x, y^i)$：因为每个 $y^i$ 是唯一的，故 $\hat{Q}^π(x,y)=\frac{\sum_{包含(x,y)的τ} G_t}{N(x,y)} = \frac{r^i}{1}=r^i$，即奖励 $r^i$ 本身
    - $\hat{A}^π(x, y^i) = \hat{Q}^π(x, y^i) - \hat{V}^π(x) = r^i - μ$

观察GRPO公式中的优势： $A(x, y^i) = \frac{ r^i - μ}{σ}$  ，可以发现就是蒙特卡洛的 $\hat{A}^π(x, y^i)$ 
  - 唯一的差别是分母 σ，相当于是为了控制方差范围做的scale（蒙特卡洛本来就有方差大的缺点），属于trick，不影响本质
  - 这里也能看出为什么计算 $A(x, y^i)$ 必须基于相同的prompt：因为 $V^π(s)$ 必须基于同一个起始状态s计算才有意义（这里是prompt x）

总结：**GRPO的优势A，本质是把prompt当成起始状态s，把整个response当成一个动作a，以同一个prompt的多个rollout作为样本池，用蒙特卡洛法估计出的优势A**

# 目标函数：策略梯度法

## RL的目标函数
RL的目标函数为： $J(θ) = \mathbb{E}_{τ \sim π_θ}[G(τ)]$  
其物理含义为：极大化轨迹的期望gain $G(τ)$。（注：如果是梯度下降，则目标函数取反变成 $-J(θ)$ 即为损失函数）
- $π_θ(a|s)$ 是策略，其中θ是待优化的参数（例如对于LLM，θ是模型权重，$π_θ$可以是第t个token的生成概率 $π_θ(y_{t}|x, y_{1:t-1})$）
- $τ \sim π_θ$ 是从 $π_θ$ 采样出的轨迹（例如LLM产生的token序列和相应reward）

## 策略梯度定理
```math
\nabla_θ J(θ)=\mathbb{E}_{τ \sim π_θ} [\sum_{t=1}^{\infty} \nabla_θ \log π_θ(a_{t+1}|s_t) \cdot G(τ)]
```
相应地，我们用右边对应的原函数 $\hat{J}(θ)$  ：
```math
\hat{J}(θ)=\mathbb{E}_{τ \sim π_θ}[\sum_{t=1}^{\infty} \log π_θ(a_{t+1}|s_t) G(τ)]
```
来代替真正的目标函数 $J(θ)$  用来实际做优化。

> 推导：  
> - 第一步：把期望展开为积分形式：  
> ```math
> J(θ) = \mathbb{E}_{τ \sim π_θ}[G(τ)] = \int_τ π_θ(τ) G(τ) d τ
> ```
> 
> - 第二步：求导，将导数转换为另一个东西的期望  
> 其中使用到变形 $\nabla_θ \log π_θ(τ) = \frac{\nabla_θ π_θ(τ)}{π_θ(τ)} \rightarrow \nabla_θ π_θ(τ) = π_θ(τ) \cdot \nabla_θ \log π_θ(τ)$
> 
> ```math
> \begin{aligned}
> \nabla_θ J(θ)
> & = \int_τ \nabla_θ π_θ(τ) G(τ) d τ \\
> & = \int_τ π_θ(τ) \nabla_θ \log π_θ(τ) G(τ) d τ \qquad (\text{where} \ \nabla_θ \log π_θ(τ) = \frac{\nabla_θ π_θ(τ)}{π_θ(τ)} )\\
> & = \mathbb{E}_{τ \sim π_θ}[\nabla_θ \log π_θ(τ) G(τ)]
> \end{aligned}
> ```
>
> - 第三步：基于马尔科夫性展开 $π_θ(τ)$
>
> ```math
> \begin{aligned}
> π_θ(τ) & = P(s_0) \cdot \prod_{t=1}^{\infty} π_θ(a_{t+1}|s_t) \cdot P(s_{t+1}|s_t, a_{t+1}) \\
> \log π_θ(τ) & = \log P(s_0) + \sum_{t=1}^{\infty} \log π_θ(a_{t+1}|s_t) + \sum_{t=1}^{\infty} \log P(s_{t+1}|s_t, a_{t+1}) \\
> \nabla_θ \log π_θ(τ)
> & = \nabla_θ \log P(s_0) + \sum_{t=1}^{\infty} \nabla_θ \log π_θ(a_{t+1}|s_t) + \sum_{t=1}^{\infty} \nabla_θ \log P(s_{t+1}|s_t, a_{t+1}) \\
> & = \sum_{t=1}^{\infty} \nabla_θ \log π_θ(a_{t+1}|s_t)
> \end{aligned}
> ```
> 其中 $\log P(s_0)$ 和 $\log P(s_{t+1}|s_t, a_{t+1})$ 因为都不含参数θ，因此对θ求导时会被消去  
>
> - 第四步：带入
> 
> ```math
> \begin{aligned}
> \nabla_θ J(θ)
> & = \mathbb{E}_{τ \sim π_θ}[\nabla_θ \log π_θ(τ) G(τ)] \\
> & = \mathbb{E}_{τ \sim π_θ}[\sum_{t=1}^{\infty} \nabla_θ \log π_θ(a_{t+1}|s_t) G(τ)]
> \end{aligned}
>```
> 
> - 第五步：新的目标函数 $\hat{J}(θ)$  
> 第四步中，右边对应的原函数其实不是 $J(θ)$ ，而是另一个原函数  $\hat{J}(θ)$
> ```math
> \hat{J}(θ)=\mathbb{E}_{τ \sim π_θ}[\sum_{t=1}^{\infty} \log π_θ(a_{t+1}|s_t) G(τ)]
> ```

注解：
- 为什么要做这么麻烦的变化？
  - 因为神经网络中没有softmax归一化的logits，天然就是 $\log π_θ(a_{t+1}|s_t)$ 的形式。
  - 例：n个候选token的logits $a_1, ..., a_n$，通过softmax转换为概率 $p_i = \frac{\exp(a_i)}{\sum \exp (a_i)}$，反过来 $a_i = \log p_i $。（严格来说是 $a_i = \log p_i + C$，C对所有 $a_i$ 是常数。但softmax有平移不变性，C可以视为0消去）
- 目标函数究竟是 $J(θ)$ 还是 $\hat{J}(θ)$？它们是一个函数吗？
  - 它们不是一个函数
  - 但是恰巧 $\hat{J}(θ)$ 和原本的目标 $J(θ)$ 求导结果相同（意味着对参数的更新效果相同）
  - 因此我们用 $\hat{J}(θ)$ 来替代真正的 $J(θ)$ 作为目标函数（这种技巧称为代理目标函数）

## 策略梯度定理的优势A形式
策略梯度定理中的 $G(τ)$ 换成 $G_t, Q(s,a), A(s,a)$ 也成立，但证明过程很麻烦（此处略过）。其中换成 **$A(s,a)$** 就是GRPO/PPO使用的公式：  

```math
\begin{aligned}
\nabla_θ J(θ) = \mathbf{\mathbb{E}_{τ \sim π_θ}[\sum_{t=1}^{\infty} \nabla_θ \log π_θ(a_{t+1}|s_t) A(s_t, a_{t+1})]}
\end{aligned}
```
相应的代理目标函数 $\hat{J}(θ)$ ：
```math
\hat{J}(θ) = \mathbf{\mathbb{E}_{τ \sim π_θ}[\sum_{t=1}^{\infty} \log π_θ(a_{t+1}|s_t) A(s_t, a_{t+1})]}
```


## 
