<h1 align="center">RL基础知识 & GRPO公式推导</h1>

# RL基础概念：S、A、R、π、环境

## 状态s、动作a、奖励r、策略π

在用强化学习训练LLM的语境中，有两种建模级别。
- token级建模：每个生成的token视为一个action（对应PPO）
- sequence级建模：每个完整生成序列视为一个action（对应GRPO）

相应的，把强化学习的概念对应到LLM上：
**token级建模**
- 状态s：
  - 初始状态 $s_1$：prompt $x$
  - t时刻状态 $s_t$：prompt $x$ + response的前t个token $y_{1:t}$
  - （*此处 $y_i$ 指的是序列 y 的第 i 个token*）
- 动作a：
  - t时刻的动作$a_t$：response的第t个token $y_{t}$
- 奖励r：
  - 最后一个token：完整序列的奖励（例如答案是否正确）
  - 前面的token：奖励为0
- 策略π：
  - LLM生成第t个token的概率 $P(y_t | x, y_{1:t-1})$

**sequence级建模（只有一问一答）**
- 状态s：只有初始、结束两个状态
  - 初始状态 $s_1$：prompt $x$
  - 结束状态 $s_2$：prompt $x$ + response $y$
- 动作a：只有一步动作
  - LLM生成的response $y$
- 奖励r：
  - 完整序列的奖励（例如答案是否正确）
- 策略π：
  - LLM生成response y的概率 $P(y | x)$

**sequence级建模（多轮问答）**：常见于多轮agent的RL训练，但也可以全拆成单轮
- 状态s：
  - 初始状态 $s_1$：初始prompt $x_{1}$
  - t时刻状态 $s_t$：初始prompt $x_{1}$ + 第1轮response $y_{1}$ + ... + 第t轮prompt $x_{t}$ + 第t轮response $y_{t}$
  - （*此处的 $x_i,y_i$ 指代的是第 i 个 x,y 序列，而不是序列中第i个token*）
- 动作a：
  - t时刻的动作 $a_t$：第t轮response $y_{t}$
- 奖励r：
  - t时刻的奖励 $r_t$：第t轮回复的奖励（有些方法有中间奖励），如果t是最后一轮则对应终局奖励
- 策略π：
  - LLM生成第t个response的概率 $P(y_t | x_1, y_1, ..., x_{t-1}, y_{t-1}, x_t)$
 
## 环境：确定性环境 & 概率环境

环境描述了给定状态s，执行动作a时，会转移到什么新状态s'，并且会获得多少奖励r。
- 有时s'和r都是确定的，则环境是确定性环境
- 有时s'和r是有随机性的，则环境是概率环境，此时需要用概率分布$P(s',r|s,a)$来描述

具体而言又可以分为s'有随机性、r有随机性等多种不同情况。以下以“步枪打靶”为例说明。

> 步枪打靶游戏：让步枪瞄准某个坐标并开枪，打中则获奖。只有一步决策，初始/结束两个状态
> - 初始状态s：步枪当前瞄准坐标 $(x_1,y_1)$
> - 动作a：“令步枪瞄准新坐标 $(x_2,y_2)$ 并开枪”的指令（把移动和开枪视为一整个动作）
> - 结束状态s'：步枪实际瞄准的新坐标 $(x'_2,y'_2)$ 
> - 奖励r：打中 $(x_2,y_2)$ 位置的靶则奖励1，否则为0
>
> 随机性的来源：
> - s'的随机性：动作执行可能不准确，命令枪瞄准$(x_2,y_2)$ 和枪实际瞄准 $(x'_2,y'_2)$ 可能并不相同
> - r的随机性：动作执行产生的效果可能不稳定，枪就算能瞄准 $(x_2,y_2)$，子弹可能会偏（瞄准了却没打中、瞄歪却打中了）
> 
> 具体解释：
> - s'随机，r确定：坏枪+好子弹，枪不一定能准确瞄准靶子，但子弹一定能准确打到瞄准的位置
> - s'确定，r随机：好枪+坏子弹，枪一定能准确瞄准靶子，但子弹可能偏到其他位置
> - s'随机，r随机：坏枪+坏子弹，枪瞄的不准，子弹落点也不准
> - s'确定，r确定（确定性环境）：好枪+好子弹在

## 强化学习中的随机性：策略随机性 vs 环境随机性

强化学习中的随机性有两个来源：
- 策略随机性 $π(a|s)$：给定状态s，以根据某种概率分布随机生成动作a。（与之相对的是最优性策略：给定s，生成某个“最优”的a）
- 环境随机性 $P(s',r|s,a)$：给状态s和动作a，转移到的新状态s'或获得的奖励r有随机性（与之相对的是确定性环境，转移到的s'和r都确定）

标准的RL各种公式（例如贝尔曼方程）中，是按策略、环境都随机给出的。
但在LLM强化学习的语境下：
- s：prompt+已经生成的token；a：新token或新response序列
- 策略随机性：对于top-p解码，策略是随机的；对于贪婪解码，策略是确定的
- 环境随机性：s无随机性（给定s和a，s'=s拼接a）；r如果由规则判断则是确定的，如果是llm-as-judge可能有随机性（本文暂不考虑这种情况）

对于LLM的强化学习（策略随机、环境确定），比标准的强化学习（策略随机、环境随机）的设定更简单，因此各种公式会有一些简化。

## 马尔科夫性
todo

# 衍生概念：G、V、A、Q

## 轨迹trace（或称episode）τ
$τ=(s_1, a_1, r_1, s_2, a_2, r_2, s_3, ..., s_n, a_n, r_n, s_{n+1})$ 从起始到结束称为一个轨迹。
- 第t步的初始状态为 $s_t$， 执行动作 $a_t$ ， 此时立刻获得奖励 $r_t$ ，并转移到下个状态 $s_{t+1}$
- 一共执行t=1...n，共n次行动
- $τ \sim π(a|s)$ 代表从状态s开始，根据策略 π(a|s) 依次采样下一个动作，直到终局，所产生的轨迹，即“从概率π(a|s)采样的轨迹τ”
  - “从状态s开始”不代表s必须是第一个状态$s_1$，它可以是任何一个中间状态 $s_t$。根据马尔科夫性， $s_t$之后的演进只和 $s_t$有关，与之前如何来到 $s_t$ 无关。

## 收益（gain）G 与折扣因子 $γ$
第t步的收益 $G_t = r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ... = \sum_{i=0}^{\infty} γ^i r_{t+i}$
- 折扣因子 0≤γ≤1
- 含义：从第t步开始直到终局，累计reward之和（越未来的步骤通常越“不重要”，因此使用折扣γ衰减。γ=0 则只看当前动作的reward，γ=1 则每一步的reward都同等重要）
- G默认是到终局位置，但也可以计算k步收益： $G_t^{(k)} = r_{t} + γ r_{t+1} + ... + γ^{k-1} r_{t+k-1} = \sum_{i=0}^{k-1} γ^i r_{t+i}$，它在k步时序差分中会用到（本文不介绍）
- 收益（gain）有时也称作回报（return），两者在强化学习中含义相同，只是不同地方的术语有差异。这里称呼为gain，防止return和reward相似混淆

## 价值（value）函数 $V^π(s)$：
对于策略π，状态s的价值为：
```math
\begin{aligned}
V^π(s) 
& = \mathbb{E}_{τ \sim π(a|s)}[G_t | S_t=s]  \\
& = \mathbb{E}_{τ \sim π(a|s)}[r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ... | S_t=s]
\end{aligned}
```
- 其中状态s执行动作a后，获得奖励r，状态变为s'
- $V^π(s)$ 和 s 处于序列的第几步无关，只和s与π有关。对于不同的策略π，价值 $V^π(s)$ 不同
- **物理意义**：从状态s按策略π(a|s)采样直到终局，**预期**能累积到多少奖励。

**辨析：奖励r、收益G、价值V**
> - 奖励reward：第t步行动**实际产生**的**即时**“收获”，衡量**状态s+动作a**的好坏
> - 收益gain：从第t步到终局**实际产生**的**累计**“收获”，衡量**轨迹τ**的好坏
> - 价值V：即从s到终局时，**预期会产生**的**累计**“收获”，衡量**状态s**的好坏

**为什么引入价值V（为什么只靠奖励r不够）？**
- reward稀疏：有些场景，中间的过程没有奖励，只有终局时才能一次性结算奖励（LLM生成过程就很典型，生成完才能评估是否正确，单个token没有奖励）
- reward好坏 ≠ 动作/状态好坏：即刻的奖励高，可能后续的奖励少，不一定是个好动作/好状态。
因此reward是“局部的、即刻的”，价值V是“全局的、考虑后续的”，它能补齐reward的视角：
- reward稀疏：没有reward时，价值V依然能评估出状态的好坏，并且根据转移到下个状态的V评估动作好坏
- reward好坏 ≠ 动作/状态好坏：考虑对未来的预期，平衡即刻的reward数值


关于价值V本身的一些澄清：**价值是客观的 → 客观但不可直接收集 → 不可收集但可以估计 → 估计有误差 → 误差可以收敛**
逐句解释：
- 价值是客观的：给定策略π和状态s，价值 $V^π(s)$ 一定是个客观实际的值。（类比：概率分布的期望是个客观实际的值）
- 客观但不可直接收集：reward和gain是可以直接收集的，但价值V是**期望**，它不能直接收集
- 不可收集但可以估计：有大量真实数据时，可以构造统计量来近似估计价值 $V^π(s)$。（类比：真实数据的平均数是个人工构造的统计量，可以近似估计概率分布的期望）
  - 动态规划法中可以为V随机赋值，并且迭代解不动点
  - PPO中用神经网络计算V
  - 蒙特卡洛中构造统计量估计V
- 估计有误差：根据真实数据估计出的价值V，数值上和客观实际的V不一定恰好相等。（类比：数据的平均数≈概率的期望，但不一定恰好相等）
- 误差可以收敛：数据足够多/迭代步数足够多，估计出的V和客观实际的V误差可忽略。（类比：数据的平均数≈概率的期望，在数据足够多时误差可忽略）

## 动作价值函数 $Q^π(s,a)$

```math
\begin{aligned}
Q^π(s,a) 
& = \mathbb{E}_{τ \sim π(a|s)}[G_t | S_t=s, A_t=a]  \\
& = \mathbb{E}_{τ \sim π(a|s)}[r_{t} + γ r_{t+1} + γ^2 r_{t+2} + ... | S_t=s, A_t=a]
\end{aligned}
```

## 贝尔曼方程：V←V、Q←Q、V←Q、Q←V 的互相递归计算

推导蒙特卡洛、GRPO不需要贝尔曼方程，此处略。推导时序差分、PPO才需要。

<details>
<summary><b>点击展开贝尔曼方程的内容</b></summary>
  
**概率环境**：
```math
\begin{aligned}
% 1. V计算V
V^π(s) & = \sum_{a} π(a|s) \sum_{s', r} P(s', r|s, a) [ r + γ V^π(s') ] \\
% 2. Q计算Q
Q^π(s, a) & = \sum_{s', r} P(s', r|s, a) [ r + γ \sum_{a'} π(a'|s') Q^π(s', a') ] \\
% 3. 用Q计算V
V^π(s) & = \sum_{a} π(a|s) Q^π(s, a) \\ % 4. 用V计算Q 
Q^π(s, a) & = \sum_{s', r} P(s', r|s, a) [ r + γ V^π(s')]
\end{aligned}
```

**确定性环境**：去掉所有 $P(s',r|s,a)$
```math
\begin{aligned}
% 1. V计算V
V^π(s) & = \sum_{a} π(a|s) [ r + γ V^π(s') ] \\
% 2. Q计算Q
Q^π(s, a) & = r + γ \sum_{a'} π(a'|s') Q^π(s', a') \\
% 3. 用Q计算V
V^π(s) & = \sum_{a} π(a|s) Q^π(s, a) \\ % 4. 用V计算Q 
Q^π(s, a) & = r + γ V^π(s')
\end{aligned}
```


</details>  

## 优势（advantage）函数 $A^π(s,a)$

$A^π(s,a) = Q^π(s,a) - V^π(s)$


```

## 回报G

回报（Return）是从某个时刻开始到episode结束的累积奖励，是智能体的长期目标。

- **数学表示**：
  $$G_t = r_{t+1} + γ r_{t+2} + γ^2 r_{t+3} + ... = \sum_{k=0}^{\infty} γ^k r_{t+k+1}$$
  
  其中 $γ \in [0, 1]$ 是折扣因子（discount factor）

- **有限horizon情况**：
  $$G_t = \sum_{k=0}^{T-t-1} γ^k r_{t+k+1}$$

- **无折扣情况**（$γ = 1$）：
  $$G_t = \sum_{k=0}^{T-t-1} r_{t+k+1}$$

## 价值V

状态价值函数（State Value Function）$V^{π}(s)$ 表示在策略 $π$ 下，从状态 $s$ 开始的期望回报。

- **数学定义**：
  $$V^{π}(s) = \mathbb{E}_{π}[G_t | s_t = s] = \mathbb{E}_{π}[\sum_{k=0}^{\infty} γ^k r_{t+k+1} \Big| s_t = s]$$

- **含义**：衡量在策略 $π$ 下，状态 $s$ 的"好坏程度"

## 优势A

优势函数（Advantage Function）$A^{π}(s, a)$ 表示在状态 $s$ 下执行动作 $a$ 相对于平均水平的优势。

- **数学定义**：
  $$A^{π}(s, a) = Q^{π}(s, a) - V^{π}(s)$$

- **含义**：
  - $A^{π}(s, a) > 0$：动作 $a$ 优于平均水平
  - $A^{π}(s, a) < 0$：动作 $a$ 劣于平均水平
  - $A^{π}(s, a) = 0$：动作 $a$ 等于平均水平

## Q函数

动作价值函数（Action-Value Function）$Q^{π}(s, a)$ 表示在策略 $π$ 下，在状态 $s$ 执行动作 $a$ 后的期望回报。

- **数学定义**：
  $$Q^{π}(s, a) = \mathbb{E}_{π}[G_t | s_t = s, a_t = a] = \mathbb{E}_{π}[\sum_{k=0}^{\infty} γ^k r_{t+k+1} \Big| s_t = s, a_t = a]$$

- **含义**：衡量在策略 $π$ 下，状态-动作对 $(s, a)$ 的"好坏程度"

- **与V函数的关系**：
  $$V^{π}(s) = \sum_{a \in \mathcal{A}} π(a|s) Q^{π}(s, a) = \mathbb{E}_{a \sim π(\cdot|s)}[Q^{π}(s, a)]$$

## 贝尔曼方程 & R/V/Q/A之间的关系

### 标准意义下（概率环境）

在概率环境中，贝尔曼方程描述了价值函数之间的递归关系：

**V函数的贝尔曼方程**：
$$V^{π}(s) = \sum_{a \in \mathcal{A}} π(a|s) \sum_{s' \in \mathcal{S}} P(s'|s, a) [R(s, a, s') + γ V^{π}(s')]$$

**Q函数的贝尔曼方程**：
$$Q^{π}(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s, a) [R(s, a, s') + γ \sum_{a' \in \mathcal{A}} π(a'|s') Q^{π}(s', a')]$$

**关系总结**：
- $Q^{π}(s, a) = \mathbb{E}_{s' \sim P(\cdot|s,a)}[R(s, a, s') + γ V^{π}(s')]$
- $V^{π}(s) = \mathbb{E}_{a \sim π(\cdot|s)}[Q^{π}(s, a)]$
- $A^{π}(s, a) = Q^{π}(s, a) - V^{π}(s)$

### 确定性环境中的简化形式

在确定性环境中，状态转移是确定的，贝尔曼方程简化为：

**V函数的贝尔曼方程**：
$$V^{π}(s) = \sum_{a \in \mathcal{A}} π(a|s) [R(s, a) + γ V^{π}(f(s, a))]$$

其中 $s' = f(s, a)$ 是确定性的状态转移函数。

**Q函数的贝尔曼方程**：
$$Q^{π}(s, a) = R(s, a) + γ \sum_{a' \in \mathcal{A}} π(a'|s') Q^{π}(s', a')$$

其中 $s' = f(s, a)$。

## 在LLM中的对应

在LLM的强化学习场景中：

- **回报G**：从当前位置到序列结束的累积奖励
  $$G_t = \sum_{k=t}^{T} r_k$$
  
  其中 $r_k$ 是对第 $k$ 个token或整个序列的奖励

- **价值V**：$V^{π}(s)$ 表示从当前文本序列 $s$ 开始的期望回报
  $$V^{π}([x_1, ..., x_t]) = \mathbb{E}_{π}[G_t | \text{当前序列}]$$

- **Q函数**：$Q^{π}(s, a)$ 表示在当前序列 $s$ 下生成token $a$ 后的期望回报
  $$Q^{π}([x_1, ..., x_t], x_{t+1}) = \mathbb{E}_{π}[G_t | \text{当前序列}, \text{下一个token}]$$

- **优势A**：$A^{π}(s, a) = Q^{π}(s, a) - V^{π}(s)$，衡量生成某个token相对于平均水平的优势

# 蒙特卡洛：估计V/A/Q

（时序差分最终演化为PPO，蒙特卡洛最终演化为GRPO，理解GRPO并不需要理解时序差分）

## 蒙特卡洛方法的基本思想

蒙特卡洛方法通过采样完整的episode来估计价值函数，不需要知道环境模型（model-free）。

### 估计V函数

通过采样多个episode，计算从状态 $s$ 开始的平均回报：

$$\hat{V}^{π}(s) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$$

其中 $G_t^{(i)}$ 是第 $i$ 个episode中从状态 $s$ 开始的回报。

### 估计Q函数

类似地，通过采样估计Q函数：

$$\hat{Q}^{π}(s, a) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$$

其中 $G_t^{(i)}$ 是第 $i$ 个episode中从状态-动作对 $(s, a)$ 开始的回报。

### 估计优势函数

优势函数可以通过Q函数和V函数计算：

$$\hat{A}^{π}(s, a) = \hat{Q}^{π}(s, a) - \hat{V}^{π}(s)$$

或者直接通过采样计算：

$$\hat{A}^{π}(s, a) = G_t - \hat{V}^{π}(s)$$

其中 $G_t$ 是从 $(s, a)$ 开始的回报，$\hat{V}^{π}(s)$ 是从状态 $s$ 的平均回报。

## 蒙特卡洛策略梯度

策略梯度方法通过直接优化策略参数来最大化期望回报。策略梯度定理给出：

$$\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim π_{\theta}} [\sum_{t=0}^{T} \nabla_{\theta} \log π_{\theta}(a_t|s_t) \cdot G_t]$$

其中 $J(\theta) = \mathbb{E}_{\tau \sim π_{\theta}}[G_0]$ 是期望回报。

### 使用优势函数

引入优势函数可以减少方差：

$$\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim π_{\theta}} [\sum_{t=0}^{T} \nabla_{\theta} \log π_{\theta}(a_t|s_t) \cdot A^{π}(s_t, a_t)]$$

因为优势函数 $A^{π}(s, a) = Q^{π}(s, a) - V^{π}(s)$ 的期望为0，所以不会改变梯度，但可以减少方差。

## 在LLM中的对应

在LLM场景中，蒙特卡洛方法的应用：

### 采样完整序列

1. **生成完整序列**：给定prompt，使用当前策略 $π_{\theta}$ 生成多个完整序列
2. **计算回报**：对每个序列计算奖励 $r$（可能只在序列结束时给出）
3. **估计价值**：通过多个样本的平均值估计价值函数

### 简化场景：确定性环境 + 单步决策

在GRPO的简化场景中：
- **确定性环境**：给定prompt和策略，生成是确定的（但在训练过程中策略会变化）
- **单步决策**：通常考虑的是生成下一个token的决策
- **回报计算**：可能基于完整序列的奖励，或者基于当前token的即时奖励

### GRPO中的蒙特卡洛估计

在GRPO中，通过以下方式估计优势：

1. **采样多个序列**：对同一prompt，生成多个完整序列
2. **计算回报**：$G_t = r$（序列级别的奖励）
3. **估计优势**：通过组内比较（group relative）来估计优势，而不是直接估计V函数

这种方法避免了需要单独估计V函数，而是通过组内相对比较来估计优势，这正是GRPO的核心思想。

# GRPO：简化场景中的蒙特卡洛

GRPO = 确定性环境 + 单步决策下的蒙特卡洛

## GRPO的核心思想

GRPO（Group Relative Policy Optimization）是一种在简化场景下的蒙特卡洛策略优化方法，特别适用于LLM的强化学习训练。

### 简化假设

1. **确定性环境**：给定prompt和策略，生成过程是确定的
2. **单步决策**：关注的是生成下一个token的决策
3. **组内比较**：通过同一prompt下的多个生成样本进行相对比较

## GRPO的数学推导

### 标准策略梯度

标准的策略梯度方法使用优势函数：

$$\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim \rho^{π}, a \sim π_{\theta}} [\nabla_{\theta} \log π_{\theta}(a|s) \cdot A^{π}(s, a)]$$

其中 $\rho^{π}$ 是状态分布。

### GRPO的优势估计

在GRPO中，优势函数通过组内相对比较来估计：

对于同一prompt $s$，生成 $K$ 个完整序列 $\{\tau_1, \tau_2, ..., \tau_K\}$，每个序列的奖励为 $\{r_1, r_2, ..., r_K\}$。

**组内平均奖励**：
$$\bar{r} = \frac{1}{K} \sum_{i=1}^{K} r_i$$

**优势估计**：
对于序列 $\tau_i$ 中的每个状态-动作对 $(s_t, a_t)$，优势估计为：

$$\hat{A}(s_t, a_t) = r_i - \bar{r}$$

这相当于用组内平均作为基线（baseline），避免了需要单独估计V函数。

### GRPO的目标函数

GRPO的目标函数可以写为：

$$L^{GRPO}(\theta) = \mathbb{E}_{s \sim \mathcal{D}} [\mathbb{E}_{\{\tau_i\}_{i=1}^{K} \sim π_{\theta}} [\frac{1}{K} \sum_{i=1}^{K} \sum_{t=0}^{T_i} \log π_{\theta}(a_t^{(i)}|s_t^{(i)}) \cdot (r_i - \bar{r})]]$$

其中：
- $\mathcal{D}$ 是prompt分布
- $K$ 是每个prompt生成的序列数量
- $T_i$ 是第 $i$ 个序列的长度
- $r_i$ 是第 $i$ 个序列的奖励
- $\bar{r} = \frac{1}{K} \sum_{j=1}^{K} r_j$ 是组内平均奖励

### 梯度计算

对参数 $\theta$ 求梯度：

$$\nabla_{\theta} L^{GRPO}(\theta) = \mathbb{E}_{s \sim \mathcal{D}} [\mathbb{E}_{\{\tau_i\}_{i=1}^{K} \sim π_{\theta}} [\frac{1}{K} \sum_{i=1}^{K} \sum_{t=0}^{T_i} \nabla_{\theta} \log π_{\theta}(a_t^{(i)}|s_t^{(i)}) \cdot (r_i - \bar{r})]]$$

### 关键特性

1. **无偏性**：由于 $\mathbb{E}[\bar{r}] = \mathbb{E}[r_i]$，使用组内平均作为基线不会引入偏差
2. **方差减少**：组内比较可以减少估计的方差
3. **无需价值函数**：不需要单独训练价值函数，简化了训练过程
4. **适合LLM**：特别适合LLM场景，因为可以自然地生成多个候选序列

## GRPO vs PPO

| 特性 | GRPO | PPO |
|------|------|-----|
| 价值估计方法 | 蒙特卡洛（组内比较） | 时序差分（价值函数） |
| 基线 | 组内平均奖励 | 价值函数 $V(s)$ |
| 优势估计 | $\hat{A} = r_i - \bar{r}$ | $\hat{A} = r + γ V(s') - V(s)$ |
| 适用场景 | 确定性环境，序列级奖励 | 一般MDP，即时奖励 |
| 训练复杂度 | 较低（无需价值网络） | 较高（需要价值网络） |

## 总结

GRPO通过以下方式简化了强化学习训练：

1. **确定性环境假设**：简化了状态转移
2. **组内相对比较**：用组内平均作为基线，避免估计V函数
3. **蒙特卡洛方法**：通过完整序列采样估计优势
4. **适合LLM**：天然适合生成多个候选序列的场景

这使得GRPO成为LLM强化学习训练的一个简洁而有效的方法。
```
